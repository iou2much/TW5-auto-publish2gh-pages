created: 20180707075829715
creator: user
modified: 20180717022242316
modifier: user
tags: supervised
title: Tree Models
tmap.edges: {"d69cbb7a-9278-472a-b461-166a068beb47":{"to":"c4f64a99-8948-45fa-be16-235a01de2145","type":"inc"},"f1110625-ed74-459d-a3c0-9a0ffe3afa36":{"to":"a7d954a3-f652-4a71-9780-b79f41c0ca4b","type":"inc"},"6458e198-5eed-42a9-868d-56ffaa1d139a":{"to":"4f390f71-147b-4c63-b207-ea8676c6a75e","type":"inc"},"17946df7-0f13-4cc3-b779-0fd077c8813c":{"to":"cc55fc64-6acf-45c3-95a1-9e5a4cd8b5b3","type":"inc"},"04fff094-6a22-4c12-a25d-4ab023eebeca":{"to":"7d8a3920-6482-4e29-acd8-a35b1892bf8a","type":"inc"}}
tmap.id: a0163f9b-22cb-436a-bb8e-3cf1a38e66f3
type: text/vnd.tiddlywiki

! 本质：

是一种贪心算法。会遍历数据构造出一棵树，这棵树上内部节点使用if-else判断数据特征，递归地把数据分到子分枝上，当一个节点满足了某种条件停止分枝时，它就成为了叶子节点，并赋值为某个数据的label。整棵树生长的过程中满足''约束''：分到子节点上的数据尽量''//纯//''。

一个完整的决策树学习算法包含有三大步骤，分别为：

　　# 分裂点特征的选择；
　　# 决策树的生成；
　　# 决策树的剪枝。
  
! Impurity

homogeneity vs heterogeneity

这里的纯是如何度量呢？有三种方式度量：
* 熵不纯度
* Gini不纯度 (计算量更低，不需要算log)
* 类不纯度

熵(Entropy)

$$ E(D) = \displaystyle-\sum_{i}p_ilog_2(p_i) $$
[img[entropy.png]]

[img[gini.png]]

ref: http://people.revoledu.com/kardi/tutorial/DecisionTree/how-to-measure-impurity.htm

叶子节点值的设定

属性缺失问题

过拟

决策树可以是一棵二叉或多叉树，它对数据的属性进行判断，得到分类或回归结果。预测时，在树的内部节点处用某一属性值（特征向量的某一分量）进行判断，根据判断结果决定进入哪个分支节点，直到到达叶子节点处，得到分类或回归结果。这是一种基于if-then-else规则的有监督学习算法，决策树的这些规则通过训练得到，而不是人工制定的。

分类树对应的映射函数是多维空间的分段线性划分，即用平行于各个坐标轴的超平面对空间进行切分；回归树的映射函数是分段常数函数。决策树是分段线性函数但不是线性函数，它具有非线性建模的能力。只要划分的足够细，分段常数函数可以逼近闭区间上任意函数到任意指定精度，因此决策树在理论上可以对任意复杂度的数据进行分类或者回归。对于分类问题，如果决策树深度够大，它可以将训练样本集的所有样本正确分类。但如果特征向量的维数过高，可能会遇到维数灾难导致准确率下降。

Key points in Tree Model:

** 判别模型
** 支持多分类

! pros & cons:

* pros:
** 解释性强
* cons:

!构建最优树(NP难)

[img[decision-tree-example.gif]]


* 类型
** 
* 如何分裂 (度量特征的贡献)
** [[CART]] ([[Gini]]) 二叉
** [[ID3]]
** [[C4.5]]
* 剪枝([[pruning]])
* 


Ref.

<<<

https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1

《数据挖掘导论》Chap. 4 -- Pang-Ning Tan

* 李宏毅老师

<<<

